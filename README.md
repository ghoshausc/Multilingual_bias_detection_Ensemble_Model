Neural models trained on large-scale corpora have tremendously aided research into open-domain conversation systems; yet, such corpora frequently pose different safety issuesÂ that severely impede the deployment of dialog systems in practice. Among all of these dangerous challenges, overcoming social prejudice is the most difficult since its detrimental impact on excluded communities is often communicated indirectly, necessitating normative reasoning and extensive study. This paper focuses on creating new biased conversations datasets in multiple languages for the research community to explore and use in their future research. 
  The paper at the same time also targets a new ensemble model that detects the language of conversation and classifies it as biased / neutral or anti-biased based on the specific language models trained on the generated dataset
